# 如何使用

OpenPAI自带一个报警系统。该系统有一些预先定义好的报警规则（alert rules）和相应的处理措施（alert actions）。管理员也可以自定义这些规则和措施。在此文档中，我们将详细介绍这个报警系统。

## Alert Rules

OpenPAI使用普罗米修斯（[Prometheus](https://prometheus.io/)）来监控整个环境，并获得监控指标（metrics）。这些指标包括内存使用率、磁盘使用率、GPU使用率等等。我们可以利用这些指标设定一些报警规则。

OpenPAI uses [`Prometheus`](https://prometheus.io/) to monitor system metrics, e.g. memory usage, disk usage, GPU usage and so on. Using the metrics, we can set up several alert rules. The alert rules define some alert conditions, and are also configured in Prometheus. When certain condition is fulfilled, Prometheus will send corresponding alert.

For example, the following configuration is the pre-defined `GpuUsedByExternalProcess` alert. It uses the metric `gpu_used_by_external_process_count`. If an external process using the GPU resource in OpenPAI over 5 minutes, Prometheus will fire a `GpuUsedByExternalProcess` alert.

``` yaml
alert: GpuUsedByExternalProcess
expr: gpu_used_by_external_process_count > 0
for: 5m
annotations:
  summary: found nvidia used by external process in {{$labels.instance}}
```

For detailed syntax of alert rules, please refer to [here](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/).

All alerts fired by the alert rules, including the pre-defined rules and the customized rules, will be shown on the home page of Webportal (on the top-right corner).

### Existing Alert Rules

By default, OpenPAI provides you with a lot of metrics and some pre-defined alert rules. You can go to `http(s)://<your master IP>/prometheus/graph` to explore different metrics. Some frequently-used metrics include:

  - `task_gpu_percent`: GPU usage percent for a single task in OpenPAI jobs
  - `task_cpu_percent`: CPU usage percent for a single task in OpenPAI jobs
  - `node_memory_MemTotal_bytes`: Total memory amount in bytes for nodes
  - `node_memory_MemAvailable_bytes`: Available memory amount in bytes for nodes

To view existing alert rules based on the metrics, you can go to `http(s)://<your master IP>/prometheus/alerts`, which includes their definitions and status.

### How to Add Customized Alerts

You can define customized alerts in the `prometheus` field in [`services-configuration.yml`](./basic-management-operations.md#pai-service-management-and-paictl). For example, We can add a customized alert `PAIJobGpuPercentLowerThan0_3For1h` by adding:

``` yaml
prometheus:
  customized-alerts: |
    groups:
      - name: customized-alerts
        rules:
          - alert: PAIJobGpuPercentLowerThan0_3For1h
            expr: avg(task_gpu_percent{virtual_cluster=~"default"}) by (job_name) < 0.3
            for: 1h
            annotations:
              summary: "{{$labels.job_name}} has a job gpu percent lower than 30% for 1 hour"
              description: Monitor job level gpu utilization in certain virtual clusters.
```

The `PAIJobGpuPercentLowerThan0_3For1h` alert will be fired when the job on virtual cluster `default` has a task level average GPU percent lower than `30%` for more than `1 hour`.
Here the metric `task_gpu_percent` is used, which describes the GPU utilization in task level. 

Remember to push service config to the cluster and restart the `prometheus` service after your modification with the following commands [in the dev-box container](./basic-management-operations.md#pai-service-management-and-paictl):
```bash
./paictl.py service stop -n prometheus
./paictl.py config push -p /cluster-configuration -m service
./paictl.py service start -n prometheus
```

Please refer to [Prometheus Alerting Rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/) for alerting rule syntax.

## Alert Actions and Routes

Admin can choose how to handle the alerts by different alert actions. We provide some basic alert actions and you can also customize your own actions. In this section, we will first introduce the existing actions and the matching rules between these actions and alerts. Then we will let you know how to add new alert actions. The actions and matching rules are both handled by [`alert-manager`](https://prometheus.io/docs/alerting/latest/alertmanager/).

### Existing Actions and Matching Rules

The alert actions and the matching rules are realized in the `alert-manager` service. To define them, you should modify the `alert-manager` field in [`services-configuration.yml`](./basic-management-operations.md#pai-service-management-and-paictl). The full spec of the configuration is as follows:

```yaml
alert-manager:
  port: 9093 # optional, do not change this if you do not want to change the port alert-manager is listening on
  alert-handler:
    port: 9095 # optional, do not change this if you do not want to change the port alert-handler is listening on
    pai-bearer-token: 'your-application-token-for-pai-rest-server'
    email-configs: # email-notification will only be enabled when this field is not empty
      admin-receiver: addr-of-admin-receiver@example.com
      smtp-host: smtp.office365.com
      smtp-port: 587
      smtp-from: alert-sender@example.com
      smtp-auth-username: alert-sender@example.com
      smtp-auth-password: password-for-alert-sender
  customized-routes: # routes are the matching rules between alerts and receivers
    routes:
    - receiver: pai-email-admin-user-and-stop-job
      match:
        alertname: PAIJobGpuPercentLowerThan0_3For1h
  customized-receivers: # receivers are combination of several actions
    - name: "pai-email-admin-user-and-stop-job"
      actions:
        - email-admin
        - email-user
        - stop-jobs
        - tag-jobs
      tags: 
        - 'stopped-by-alert-manager'

```

We have provided so far these following actions: 

  - `email-admin`: Send emails to the assigned admin.
  - `email-user`: Send emails to the owners of jobs.
  - `stop-jobs`: Stop jobs by calling OpenPAI REST API.
  - `tag-jobs`: Add a tag to jobs by calling OpenPAI REST API.
  - `cordon-nodes`: Call Kubernetes API to cordon the corresponding nodes.

But before you use them, you have to add proper configuration in the `alert-handler` field. For example, `email-admin` needs you to set up an SMTP account to send the email and an admin email address to receive the email. Also, the `tag-jobs` and `stop-jobs` action calls OpenPAI REST API, so you should set a rest server token for them. To get the token, you should go to your profile page (in the top-right corner on Webporal, click `View my profile`), and use `Create application token` to create one. Generally speaking, there are two parts of configuration in the `alert-handler` field. One is `email-configs`. The other is `pai-bearer-token`. The requirements for different actions are shown in the following table:

|              | email-configs | pai-bearer-token |
| :-----------:| :-----------: | :--------------: |
| cordon-nodes | -             | -                |
| email-admin  | required      | -                |
| email-user   | required      | required         |
| stop-jobs    | -             | required         |
| tag-jobs     | -             | required         |

In addition, some actions may depend on certain field in the `labels` of alert instance. The labels of the `alert instance` are generated based on the expression in the alert rule. For example, the expression of the `PAIJobGpuPercentLowerThan0_3For1h` alert we mentioned in previous section is `avg(task_gpu_percent{virtual_cluster=~"default"}) by (job_name) < 0.3`. This expression returns a list, the element in which contains the `job_name` field. Sothere will be also a `job_name` field in the labels of the alert instance. `stop-jobs` action depends on the `job_name` field, and it will stop the corresponding job based on it. To inspect the labels of an alert, you can visit `http(s)://<your master IP>/prometheus/alerts`. If the alert is firing, you can see its labels on this page. For the depended fields of each pre-defined action, please refer to the following table:

|              | depended label field |
| :-----------:| :------------------: |
| cordon-nodes | node_name            |
| email-admin  | -                    | 
| email-user   | -                    |
| stop-jobs    | job_name             |
| tag-jobs     | job_name             |


The matching rules between alerts and actions are defined using `receivers` and `routes`.
A `receiver` is simply a group of actions, a `route` matches the alerts to a specific `receiver`. 

With the default configuration, all the alerts will match the default alert receiver which triggers only `email-admin` action (But if you don't set the email configuration, the action won't work).
You can add new receivers with related matching rules to assign actions to alerts in the `alert-manager` field in [`service-configuration.yml`](./basic-management-operations.md#pai-service-management-and-paictl).


For example :

``` yaml
alert-manager:
  ......
  customized-routes: # routes are the matching rules between alerts and receivers
    routes:
    - receiver: pai-email-admin-user-and-stop-job
      match:
        alertname: PAIJobGpuPercentLowerThan0_3For1h
  customized-receivers: # receivers are combination of several actions
    - name: "pai-email-admin-user-and-stop-job"
      actions: # We have provided so far these actions: email-admin, email-user, stop-jobs, tag-jobs
        - email-admin
        - email-user
        - stop-jobs
        - tag-jobs
      tags: 
        - 'stopped-by-alert-manager'
  ......
```

Here we define:

- a receiver `pai-email-admin-user-and-stop-job`, which contains the actions `email-admin`, `email-user`, `stop-jobs` and `tag-jobs`
- a route, which matches the alert `pai-email-admin-user-and-stop-job` to the receiver `pai-email-admin-user-and-stop-job`.

As a consequence, when the alert `PAIJobGpuPercentLowerThan0_3For1h` is fired, all these 4 actions will be triggered.

For `routes` definition, we adopt the syntax of [Prometheus Alertmanager](https://prometheus.io/docs/alerting/latest/configuration/).
For `receivers` definition, you can simply:

- name the receiver in `name` field;
- list the actions to use in `actions`;
- list the tags in `tags` if `tag-jobs` is one of the actions.

Remember to push service config to the cluster and restart the `alert-manager` service after your modification with the following commands in the dev-box container:

```bash
./paictl.py service stop -n alert-manager
./paictl.py config push -p /cluster-configuration -m service
./paictl.py service start -n alert-manager
```
For OpenPAI service management, please refer to [here](./basic-management-operations.md#pai-service-management-and-paictl).

### 如何添加自定义的处理措施

如果您想要添加自定义的处理措施，请参考下面的步骤：

####  在'alert-handler'中实现相应的措施

`alert-handler`是一个轻量的`express`应用，您可以在其中方便地添加一些自定义的API。

例如，`stop-jobs`这个处理措施实际是通过普罗米修斯的`webhook`调用`localhost:9095/alert-handler/stop-jobs`来实现的。在`alert-handler`内部，这个请求被转发给OpenPAI的REST API，来结束相应的任务。您可以再`alert-handler`中添加新的API，来实现新的处理措施。

`alert-handler`的源码位置在[这里](https://github.com/microsoft/pai/blob/master/src/alert-manager/src/alert-handler)。

#### 检查处理措施的依赖

正如我们之前所说的，想要让一个处理措施可用，管理员需要添加相应的配置。

请检查[这个文件夹](https://github.com/microsoft/pai/tree/master/src/alert-manager/config)， 并且为您新添加的处理措施指定它的依赖规则。

#### 在Webhook的定义中添加该处理措施

在`service-configuration.yml`中定义`receiver`的时候，实际我们会把对应的处理措施渲染到[这里](https://github.com/microsoft/pai/blob/master/src/alert-manager/deploy/alert-manager-configmap.yaml.template)的webhook_configs中。

所有我们提供的处理措施，即`email-admin`, `email-user`, `stop-jobs`, `tag-jobs`和`cordon-nodes`，在`alert-manager`中会发送相应的POST请求到`alert-handler`，对应的URL如下：

- `localhost:{your_alert_handler_port}/alert-handler/send-email-to-admin`
- `localhost:{your_alert_handler_port}/alert-handler/send-email-to-user`
- `localhost:{your_alert_handler_port}/alert-handler/stop-jobs`
- `localhost:{your_alert_handler_port}/alert-handler/tag-jobs/:tag`
- `localhost:{your_alert_handler_port}/alert-handler/cordon-nodes`

发送POST请求时，请求的body是由`alert-manager`自动填写的。具体的处理措施将会在`alert-handler`内部运行。

请在[这里](https://github.com/microsoft/pai/blob/master/src/alert-manager/src/alert-handler)添加您处理措施名称到`alert-handler` URL路径的渲染规则。

以上步骤全部完成后，记得在dev box容器中重新build和push Docker镜像，并在重启`alert-manager`服务。

```bash
./build/pai_build.py build -c /cluster-configuration/ -s alert-manager
./build/pai_build.py push -c /cluster-configuration/ -i alert-handler
./paictl.py service stop -n alert-manager
./paictl.py config push -p /cluster-configuration -m service
./paictl.py service start -n alert-manager
```
